{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2.4: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Models and Losses\n",
    "\n",
    "Model: $f_{\\theta}$&ensp;&ensp;&ensp;&ensp;$L(\\theta)\\ =\\ L(\\theta\\ |\\ \\mathcal{D})\\ =\\ \\mathbb{E}_{(\\text{x},\\ \\text{y})\\sim\\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ \\text{y})]$\n",
    "\n",
    "Parameters: $\\theta\\ =\\ (\\text{W},\\ \\text{b})$ </br>\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\text{Wx}\\ +\\ \\text{b}\\ \\rightarrow\\ y$\n",
    "\n",
    "* Low Loss - good\n",
    "* High Loss - bad\n",
    "* Loss function over the full dataset $\\mathcal{D}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Model\n",
    "\n",
    "Find: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta^{*}\\ =\\ \\underset{\\theta}{\\text{arg min}}\\ L(\\theta\\ |\\ \\mathcal{D})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Deep Learning uses **gradient descent**:\n",
    "* Updates parameters via the negative gradient\n",
    "* Iterative method\n",
    "\n",
    "Update rule: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta'\\ =\\ \\theta\\ -\\ \\epsilon[\\nabla_{\\theta}L(\\theta)]^{\\top}$\n",
    "\n",
    "where $\\epsilon$ is the learning rate\n",
    "\n",
    "**Pseudocode**\n",
    "\n",
    "for iteration in range(n):\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;J = $\\nabla L(\\theta)$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ \\theta\\ -\\ \\epsilon * \\text{J.mT}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
