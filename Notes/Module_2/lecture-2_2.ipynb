{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2.2: Datasets and Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting\n",
    "\n",
    "Goal: find parameters $\\theta$ &ensp;&ensp;&ensp;&ensp;$f_{\\theta}(\\text{x})\\ =\\ \\text{Wx}\\ +\\ b$\n",
    "\n",
    "Components:\n",
    "* dataset\n",
    "* loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Recap:\n",
    "* samples from a data generating distribution\n",
    "* we use \\textbf{labels} to train the model\n",
    "\n",
    "$\\mathcal{D}\\ =\\ \\{(\\text{x}_{i},\\ \\text{y}_{i})\\}_{i\\ =\\ 1}^{N} \\text{where}\\ (\\text{x}_{i},\\ \\text{y}_{i}) \\sim P(\\text{X},\\ \\text{Y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "A \\textbf{loss function} measures the quality of the model\n",
    "\n",
    "Loss Function:\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x}_{i},\\ \\text{y}_{i})$\n",
    "\n",
    "Expected Loss:\n",
    "&ensp;&ensp;&ensp;&ensp;$L(\\theta\\ |\\ \\mathcal{D})\\ =\\ \\mathbb{E}_{(\\text{x},\\ \\text{y})\\ \\sim\\ \\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ \\text{y})]$\n",
    "\n",
    "* $l(\\theta\\ |\\ \\text{x}_{i},\\ \\text{y}_{i})$ measures how good the prediction of our model is on $1\\text{X}1$ single data item\n",
    "* $L(\\theta\\ |\\ \\mathcal{D})\\ =\\ \\mathbb{E}_{(\\text{x},\\ \\text{y})\\ \\sim\\ \\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ \\text{y})]$ takes the entire data set and computes the average of this loss for each individual data element (a.k.a. expected loss)\n",
    "* In practice, always optimize the expected loss over the entire data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of a Loss Function\n",
    "\n",
    "* **Low Loss** - good\n",
    "* **High Loss** - bad\n",
    "* Loss function over the full dataset $\\mathcal{D}$ </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$L(\\theta)\\ =\\ L(\\theta\\ |\\ \\mathcal{D})\\ =\\ \\mathbb{E}_{(\\text{x},\\ \\text{y})\\ \\sim\\ \\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ \\text{y})]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function: Examples\n",
    "\n",
    "Linear Regression: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\hat{\\text{y}}\\ =\\ \\text{Wx}\\ +\\ \\text{b}$\n",
    "\n",
    "L2 Loss: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ \\frac{1}{2}||\\hat{\\text{y}}\\ -\\ \\text{y}||_{2}^{2}$\n",
    "\n",
    "* Measured either by Euclidean or set of L1 distances between model predictions and expected predictions\n",
    "* Can think of as just the distance between two points in space\n",
    "* One is the prediction of the model and the other is the ground truth label\n",
    "\n",
    "Binary Classification: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\hat{\\text{y}}\\ =\\ \\sigma(\\text{Wx}\\ +\\ \\text{b})$\n",
    "\n",
    "Sigmoid + Binary Cross Entropy Loss: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ -\\text{y log}(\\hat{\\text{y}})\\ -\\ (1\\ -\\ \\text{y})\\text{log}(1\\ -\\ \\hat{\\text{y}})$\n",
    "\n",
    "* Output of the model is a transformation of the linear model through a sigmoid\n",
    "* Sigmoid gives us a probability if a certain input or output is closer to class one vs class zero\n",
    "* Binary Cross Entropy is the negative log likelihood of the sigmoid after right class\n",
    "* $-(1\\ -\\ \\text{y})\\text{log}(1\\ -\\ \\hat{\\text{y}})$ means that if the label for a certain data point is one, we look at the top part $-\\text{y log}(\\hat{\\text{y}})$\n",
    "* This will optimize the log of $\\hat{\\text{y}}$ which is the log likelihood of the model or binary classification of the model\n",
    "* If the label of $-\\text{y log}(\\hat{\\text{y}})\\ =\\ 0$ we only look at the bottom part $-(1\\ -\\ \\text{y})\\text{log}(1\\ -\\ \\hat{\\text{y}})$ and we maximize $1\\ -\\ \\text{log}$ of the probability that the model predicts\n",
    "* All this means is that we always want to maximize the log probability of the correct label\n",
    "\n",
    "Multi-Class Classification: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\hat{\\text{y}}\\ =\\ \\text{softmax}(\\text{Wx}\\ +\\ \\text{b})$\n",
    "\n",
    "Softmax + Cross Entropy Loss: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ -\\sum\\limits_{c\\ =\\ 1}^{C}1_{[y\\ =\\ c]}\\text{log}(\\hat{\\text{y}}_{c})$\n",
    "\n",
    "* Softmax will contain multiple outputs; will produce a vector of probabilities over n classes\n",
    "* The Cross Entropy Loss says that one of the classes is going to be correct\n",
    "* For the correct class, Cross Entropy Loss maximizes the log likelihood of the softmax of the correct class\n",
    "* $\\text{log}(\\hat{\\text{y}}_{\\text{c}})$ is the log of the correct class\n",
    "* $\\text{log}(\\hat{\\text{y}}_{\\text{c}})$ can be rewritten for optimization as $-\\text{log}(\\hat{\\text{y}}_{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Loss\n",
    "\n",
    "Converts a sample-based loss $l$ into dataset-based loss $L$ </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$L(\\theta\\ |\\ \\mathcal{D})\\ =\\ \\mathbb{E}_{(\\text{x},\\ \\text{y})\\sim\\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ \\text{y})]$\n",
    "* $l(\\theta\\ |\\ \\text{x},\\ \\text{y})$ function of $\\theta,\\ \\text{x},\\ \\text{y}$\n",
    "* $L(\\theta\\ |\\ \\mathcal{D})$ function of $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next - Model Fitting\n",
    "\n",
    "Goal: find parameters $\\theta$ </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta^{*}\\ =\\ \\underset{\\theta}{\\text{arg min}}L(\\theta\\ |\\ \\mathcal{D})$\n",
    "\n",
    "Deep learning uses **gradient descent**:\n",
    "* requires gradient $\\nabla_{\\theta}L(\\theta\\ |\\ \\mathcal{D})$\n",
    "* requires optimizers to update $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
