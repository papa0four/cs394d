{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2.5: Computational Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Gradients\n",
    "\n",
    "#### Gradient of simple functions: Fine to compute\n",
    "&ensp;&ensp;&ensp;&ensp;$\\nabla_{\\text{w}} L(\\theta\\ |\\ \\mathcal{D})$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$=\\ \\nabla_{\\text{w}} E_{\\text{x},\\ \\text{y}\\ \\sim\\ \\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ y)]$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$=\\ E_{\\text{x},\\ \\text{y}\\ \\sim\\ \\mathcal{D}}[\\nabla_{\\text{w}}l(\\theta\\ |\\ \\text{x},\\ y)]$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$=\\ E_{\\text{x},\\ \\text{y}\\ \\sim\\ \\mathcal{D}}[\\nabla_{\\text{w}}(\\text{w}^{\\top}\\text{x}\\ +\\ b\\ -\\ y)^{2}]$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$=\\ 2E_{\\text{x},\\ \\text{y}\\ \\sim\\ \\mathcal{D}}[(\\text{w}^{\\top}\\text{x}\\ +\\ b\\ -\\ y)\\nabla_{\\text{w}}\\text{w}^{\\top}\\text{x}]$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$=\\ 2E_{\\text{x},\\ \\text{y}\\ \\sim\\ \\mathcal{D}}[(\\text{w}^{\\top}\\text{x}\\ +\\ b\\ -\\ y)\\text{x}^{\\top}]$\n",
    "\n",
    "#### Gradient of regular functions: Quickly get complicated\n",
    "General Linear Regression Model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ (\\text{Wx}\\ +\\ \\text{b}\\ -\\ \\text{y})^{2}$\n",
    "\n",
    "Binary Logistic Regression: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ y\\ \\text{log}\\ \\sigma(\\text{Wx}\\ +\\ \\text{b})\\ +\\ (1\\ -\\ \\text{y})\\ \\text{log}(1\\ -\\ \\sigma(\\text{Wx}\\ +\\ \\text{b}))$\n",
    "\n",
    "Multi-class Logistic Regression: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ \\text{log softmax}(\\text{Wx}\\ +\\ \\text{b})_{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation as a Graph\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x, y})\\ = \\ \\text{log}(\\text{softmax}(\\text{Wx}\\ +\\ \\text{b}))_{y}$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\ \\text{index}\\ (\\text{log}\\ (\\text{softmax}\\ (\\text{add}\\ (\\text{matmul}\\ (\\text{W, x}),\\ \\text{b}))),\\ y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient and Chain Rule\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x, y})\\ =\\ \\text{index}\\ (\\text{log}\\ (\\text{softmax}\\ (\\text{add}\\ (\\text{matmul}(\\text{W, x}), \\text{b}))), y)$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\nabla_{\\theta} l(\\theta\\ |\\ \\text{x, y}) = \\nabla_{\\theta}\\text{index}\\ (\\text{log}\\ (\\text{softmax}\\ (\\text{add}\\ (\\text{matmul}(\\text{W, x}), \\text{b}))), y)$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\ \\underbrace{\\frac{\\delta}{\\delta\\ \\text{log}}\\ \\text{index}}_{\\nabla\\ \\text{index}}(.\\ .\\ .)\\ \\nabla_{\\theta}\\ \\text{log}\\ (\\text{softmax}\\ (\\text{add}\\ (\\text{matmul}(\\text{W, x}),\\ \\text{b})))$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\ \\nabla\\ \\text{index}(.\\ .\\ .)\\ \\nabla\\ \\text{log}(.\\ .\\ .)\\ \\nabla_{\\theta}\\ \\text{softmax}\\ (\\text{add}\\ (\\text{matmul}(\\text{W, x}), \\text{b}))$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\ \\nabla\\ \\text{index}(.\\ .\\ .)\\ \\nabla\\ \\text{log}(.\\ .\\ .)\\ \\nabla\\ \\text{softmax}(.\\ .\\ .)\\ \\nabla\\ \\text{add}(.\\ .\\ .)\\ (\\nabla\\ \\text{matmul}(\\text{W, x})\\ \\nabla_{\\theta}\\ \\text{W}\\ +\\ \\nabla_{\\theta}\\text{b})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients - Direction of Evaluation\n",
    "\n",
    "$$\n",
    "\\underbrace{\\nabla_{\\theta}l(\\theta\\ |\\ \\text{x, y})}_{\\mathbb{R}^{n}}\\ =\\ \\underbrace{\\nabla\\ \\text{index}(.\\ .\\ .)}_{\\mathbb{R}^{n}}\\underbrace{\\nabla\\ \\text{log}(.\\ .\\ .)}_{\\mathbb{R}^{n\\ \\times\\ m}}\\underbrace{\\nabla\\ \\text{softmax}(.\\ .\\ .)}_{\\mathbb{R}^{m\\ \\times\\ l}}\\underbrace{\\nabla\\ \\text{add}(.\\ .\\ .)}_{\\mathbb{R}^{l\\ \\times\\ k}}\\ \\left(\\underbrace{\\nabla\\ \\text{matmul}(\\text{W, x})\\nabla_{\\theta}\\text{W}}_{\\mathbb{R}^{k\\ \\times\\ ...}}\\ +\\ \\underbrace{\\nabla_{\\theta}\\text{b}}_{\\mathbb{R}^{k\\ \\times\\ ...}}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients - Backpropagation\n",
    "\n",
    "Gradients computed backwards in graph\n",
    "* Computationally more efficient\n",
    "* One backward pass computes gradients of **all** parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients: Backpropagation in Practice\n",
    "\n",
    "Each operation in PyTorch\n",
    "* Has backward-function implemented\n",
    "* Graph constructed automatically\n",
    "\n",
    "Backward pass\n",
    "* Multiplies vector with Jacobian of operator\n",
    "* Start by back-propagating value of 1 to loss\n",
    "* Can only call backward on scalars\n",
    "* Populates `Tensor.grad` for any tensor that `requires_grad=True`\n",
    "\n",
    "```python\n",
    "a = torch.rand(100, requires_grad=True)\n",
    "b = 0.5 * (a**2).sum()\n",
    "b.backward()\n",
    "a.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graphs TL;DR\n",
    "\n",
    "PyTorch builds computational graphs for automatic differentiation\n",
    "\n",
    "Gradients are propagated backward through computational graphs: backpropagation\n",
    "\n",
    "Call `Tensor.backward()` in PyTorch\n",
    "\n",
    "No more compicated gradient math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
