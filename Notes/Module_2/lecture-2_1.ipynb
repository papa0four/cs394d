{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2.1: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Regression Model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{R}^{d}$\n",
    "\n",
    "Linear Regression: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(x)\\ =\\ Wx\\ +\\ b$\n",
    "\n",
    "Parameters: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ (W,\\ b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Exammple:\n",
    "\n",
    "Temperature Forecast:\n",
    "\n",
    "$f(x)$: average temperature on day $x$\n",
    "\n",
    "Day as input, temperature as output where the weight determines the slope of the line and the bias determines how much the line shifts up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(10, 1) # always define this linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=1, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2771, -0.1729,  0.0368,  0.2826, -0.2734, -0.1930, -0.0183, -0.0631,\n",
      "          0.1511,  0.3114]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2628], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2628], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = torch.ones(10)\n",
    "y = torch.zeros(10)\n",
    "# model(x)\n",
    "model(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression in PyTorch\n",
    "\n",
    "Define a linear regression model:\n",
    "\n",
    "```python\n",
    "linear = torch.nn.Linear(4, 2)\n",
    "print(f\"{linear.weight=}\")\n",
    "print(f\"{linear.bias=}\")\n",
    "\n",
    "x = torch.as_tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "print(f\"{linear(x)==}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Limitation\n",
    "\n",
    "Cannot deal with non-linear patterns:\n",
    "* cyclic functions\n",
    "* quadratic functions\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification\n",
    "\n",
    "Binary classification model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ [0,\\ 1]$\n",
    "\n",
    "Linear binary classfication: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(x)\\ =\\ \\sigma(Wx\\ +\\ b)$ </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\sigma(x)\\ =\\ \\frac{1}{1\\ +\\ e^{-x}}$\n",
    "\n",
    "Parameters: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ (W,\\ b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification: Decision Boundary\n",
    "\n",
    "Same exact function as the linear regression model, just wrapped in the sigmoid function: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\sigma(Wx\\ +\\ b)$\n",
    "\n",
    "* The weight $(W)$ determined the rotation of the decision boundary that separates the two classes\n",
    "* The bias $(b)$ is how far we shift the plane up or down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification: An Example\n",
    "\n",
    "Input $x$: average daily temperature </br>\n",
    "Output $f(x)$: whether it will rain on Wednesday\n",
    "\n",
    "Prediction: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$P(rain)\\ =\\ f_{\\theta}\\ =\\ \\sigma(Wx\\ +\\ b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (fc): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([[-0.2464, -0.3095, -0.0098, -0.3001,  0.0657,  0.1902,  0.3049,  0.1928,\n",
      "         -0.0161,  0.2664]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0614], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4595],\n",
       "        [0.4951],\n",
       "        [0.5111],\n",
       "        [0.5644],\n",
       "        [0.5526],\n",
       "        [0.5000],\n",
       "        [0.4993],\n",
       "        [0.4741],\n",
       "        [0.5962],\n",
       "        [0.5643],\n",
       "        [0.5184],\n",
       "        [0.5854],\n",
       "        [0.5774],\n",
       "        [0.4447],\n",
       "        [0.4841],\n",
       "        [0.4898],\n",
       "        [0.5456],\n",
       "        [0.4944],\n",
       "        [0.5013],\n",
       "        [0.5198],\n",
       "        [0.4783],\n",
       "        [0.5244],\n",
       "        [0.5278],\n",
       "        [0.4887],\n",
       "        [0.5849],\n",
       "        [0.5138],\n",
       "        [0.4937],\n",
       "        [0.5652],\n",
       "        [0.5292],\n",
       "        [0.5430],\n",
       "        [0.5397],\n",
       "        [0.5353],\n",
       "        [0.4708],\n",
       "        [0.6335],\n",
       "        [0.4633],\n",
       "        [0.4829],\n",
       "        [0.5538],\n",
       "        [0.5512],\n",
       "        [0.5572],\n",
       "        [0.5787],\n",
       "        [0.5576],\n",
       "        [0.5796],\n",
       "        [0.5577],\n",
       "        [0.6657],\n",
       "        [0.5417],\n",
       "        [0.4775],\n",
       "        [0.5007],\n",
       "        [0.5625],\n",
       "        [0.4658],\n",
       "        [0.5454],\n",
       "        [0.5207],\n",
       "        [0.5249],\n",
       "        [0.5864],\n",
       "        [0.5058],\n",
       "        [0.5123],\n",
       "        [0.5514],\n",
       "        [0.5593],\n",
       "        [0.5772],\n",
       "        [0.4760],\n",
       "        [0.4183],\n",
       "        [0.6361],\n",
       "        [0.5009],\n",
       "        [0.5096],\n",
       "        [0.4645],\n",
       "        [0.6184],\n",
       "        [0.5285],\n",
       "        [0.5396],\n",
       "        [0.4612],\n",
       "        [0.5893],\n",
       "        [0.4855],\n",
       "        [0.4585],\n",
       "        [0.4684],\n",
       "        [0.5997],\n",
       "        [0.5136],\n",
       "        [0.5200],\n",
       "        [0.5214],\n",
       "        [0.6197],\n",
       "        [0.6303],\n",
       "        [0.4973],\n",
       "        [0.5097],\n",
       "        [0.5154],\n",
       "        [0.4314],\n",
       "        [0.6325],\n",
       "        [0.5198],\n",
       "        [0.5031],\n",
       "        [0.5291],\n",
       "        [0.4673],\n",
       "        [0.4571],\n",
       "        [0.5241],\n",
       "        [0.5870],\n",
       "        [0.5732],\n",
       "        [0.6274],\n",
       "        [0.5424],\n",
       "        [0.5047],\n",
       "        [0.5146],\n",
       "        [0.5805],\n",
       "        [0.5273],\n",
       "        [0.5902],\n",
       "        [0.5234],\n",
       "        [0.5605]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Never put a sigmoid directly into your model.\"\"\"\n",
    "        return nn.functional.sigmoid(self.fc(x))\n",
    "    \n",
    "model = LinearClassifier(10, 1)\n",
    "print(model)\n",
    "print(model.fc.weight)\n",
    "print(model.fc.bias)\n",
    "\n",
    "# x = torch.zeros(10)\n",
    "# model(x)\n",
    "# x = torch.ones(10)\n",
    "# model(x)\n",
    "x = torch.rand(100, 10)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification: Limitation\n",
    "\n",
    "* Linear classifier are not very powerful models\n",
    "* Cannot deal with non-linear decision boundaries\n",
    "* For deep learning, we will need to use non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Multi-Class Classification\n",
    "\n",
    "Multi-class classification model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{P}^{c}\\ \\text{where}\\ \\mathbb{P}^{c}\\ \\subset\\ \\mathbb{R}_{+}^{c}\\ \\ \\forall_{y\\ \\in\\ \\mathbb{P}^{c}}1^{\\top}y\\ =\\ 1$ </br>\n",
    "* Input: Real valued number\n",
    "* Output: One class in $C$ possible classes\n",
    "\n",
    "Linear multi-class classification: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(x) = \\text{softmax}(Wx\\ +\\ b)$\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{softmax}(v)_{i}\\ =\\ \\frac{e^{V_{i}}}{\\Sigma_{j}^{n}\\ =\\ 1^{e^{v_{j}}}}$ </br>\n",
    "* $C$ real valued numbers that are all positive\n",
    "* One additional constraint: There are $C$ positive numbers that all sum up to one\n",
    "* Softmax: a function that transformed input to binary classification probabilities for multiple classes\n",
    "\n",
    "Parameters: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ (W,\\ b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, always regress to an output size $C$, where $C$ is the number of classes we want to split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "For input v =\n",
    "\\begin{bmatrix}\n",
    "V_{1} \\\\\n",
    "... \\\\\n",
    "V_{d}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{d},\\ \\text{functions softmax}\\ :\\ \\mathbb{R}^n\\ \\rightarrow\\ \\mathbb{P}^{c}.\n",
    "\n",
    "$\\mathbb{P}^{c}\\ \\subset\\ \\mathbb{R}_{+}^{c}$ &ensp;&ensp;&ensp;&ensp;$\\forall_{y\\ \\in\\ \\mathbb{P}^{c}}1^{\\top}y\\ =\\ 1$ </br>\n",
    "$$\n",
    "\\text{softmax}(\\text{v})\\ =\\ \\frac{1}{\\Sigma_{i}\\ e^{v_{i}}}\n",
    "\\begin{bmatrix}\n",
    "e^{v_{1}} \\\\\n",
    "... \\\\\n",
    "e^{v_{d}}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
