{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2.1: Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "Regression Model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{R}^{d}$\n",
    "\n",
    "Linear Regression: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(x)\\ =\\ Wx\\ +\\ b$\n",
    "\n",
    "Parameters: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ (W,\\ b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Exammple:\n",
    "\n",
    "Temperature Forecast:\n",
    "\n",
    "$f(x)$: average temperature on day $x$\n",
    "\n",
    "Day as input, temperature as output where the weight determines the slope of the line and the bias determines how much the line shifts up or down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(10, 1) # always define this linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "print(model.weight)\n",
    "print(model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones(10)\n",
    "y = torch.zeros(10)\n",
    "# model(x)\n",
    "model(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression in PyTorch\n",
    "\n",
    "Define a linear regression model:\n",
    "\n",
    "```python\n",
    "linear = torch.nn.Linear(4, 2)\n",
    "print(f\"{linear.weight=}\")\n",
    "print(f\"{linear.bias=}\")\n",
    "\n",
    "x = torch.as_tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "print(f\"{linear(x)==}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression: Limitation\n",
    "\n",
    "Cannot deal with non-linear patterns:\n",
    "* cyclic functions\n",
    "* quadratic functions\n",
    "* ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification\n",
    "\n",
    "Binary classification model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ [0,\\ 1]$\n",
    "\n",
    "Linear binary classfication: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(x)\\ =\\ \\sigma(Wx\\ +\\ b)$ </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\sigma(x)\\ =\\ \\frac{1}{1\\ +\\ e^{-x}}$\n",
    "\n",
    "Parameters: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ (W,\\ b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification: Decision Boundary\n",
    "\n",
    "Same exact function as the linear regression model, just wrapped in the sigmoid function: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\sigma(Wx\\ +\\ b)$\n",
    "\n",
    "* The weight $(W)$ determined the rotation of the decision boundary that separates the two classes\n",
    "* The bias $(b)$ is how far we shift the plane up or down"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification: An Example\n",
    "\n",
    "Input $x$: average daily temperature </br>\n",
    "Output $f(x)$: whether it will rain on Wednesday\n",
    "\n",
    "Prediction: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$P(rain)\\ =\\ f_{\\theta}\\ =\\ \\sigma(Wx\\ +\\ b)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Never put a sigmoid directly into your model.\"\"\"\n",
    "        return nn.functional.sigmoid(self.fc(x))\n",
    "    \n",
    "model = LinearClassifier(10, 1)\n",
    "print(model)\n",
    "print(model.fc.weight)\n",
    "print(model.fc.bias)\n",
    "\n",
    "# x = torch.zeros(10)\n",
    "# model(x)\n",
    "# x = torch.ones(10)\n",
    "# model(x)\n",
    "x = torch.rand(100, 10)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Binary Classification: Limitation\n",
    "\n",
    "* Linear classifier are not very powerful models\n",
    "* Cannot deal with non-linear decision boundaries\n",
    "* For deep learning, we will need to use non-linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Multi-Class Classification\n",
    "\n",
    "Multi-class classification model: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{P}^{c}\\ \\text{where}\\ \\mathbb{P}^{c}\\ \\subset\\ \\mathbb{R}_{+}^{c}\\ \\ \\forall_{y\\ \\in\\ \\mathbb{P}^{c}}1^{\\top}y\\ =\\ 1$ </br>\n",
    "* Input: Real valued number\n",
    "* Output: One class in $C$ possible classes\n",
    "\n",
    "Linear multi-class classification: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(x) = \\text{softmax}(Wx\\ +\\ b)$\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{softmax}(v)_{i}\\ =\\ \\frac{e^{V_{i}}}{\\Sigma_{j}^{n}\\ =\\ 1^{e^{v_{j}}}}$ </br>\n",
    "* $C$ real valued numbers that are all positive\n",
    "* One additional constraint: There are $C$ positive numbers that all sum up to one\n",
    "* Softmax: a function that transformed input to binary classification probabilities for multiple classes\n",
    "\n",
    "Parameters: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\theta\\ =\\ (W,\\ b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For multi-class classification, always regress to an output size $C$, where $C$ is the number of classes we want to split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "$$\n",
    "\\text{For input v}\\ =\n",
    "\\begin{bmatrix}\n",
    "V_{1} \\\\\n",
    "... \\\\\n",
    "V_{d}\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{d},\\ \\text{functions softmax}\\ :\\ \\mathbb{R}^n\\ \\rightarrow\\ \\mathbb{P}^{c}.\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{P}^{c}\\ \\subset\\ \\mathbb{R}_{+}^{c}\\ \\ \\ \\ \\forall_{y\\ \\in\\ \\mathbb{P}^{c}}1^{\\top}y\\ =\\ 1\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\text{v})\\ =\\ \\frac{1}{\\Sigma_{i}\\ e^{v_{i}}}\n",
    "\\begin{bmatrix}\n",
    "e^{v_{1}} \\\\\n",
    "... \\\\\n",
    "e^{v_{d}}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "* Input: $\\mathbb{R}$ Real values range from $-\\infty\\ \\text{to}\\ \\infty$\n",
    "* Output: The probability $\\mathbb{P}$ over all possible classes\n",
    "* Softmax exponentializes all the input, taking any input from $-\\infty\\ \\text{to}\\ \\infty$ converting it between $0\\ \\text{to}\\ \\infty$\n",
    "* Next, the values are normalized to sum up to $1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Multi-Class Classification\n",
    "\n",
    "$$\n",
    "\\text{Let W}\\ =\\ \n",
    "\\begin{bmatrix}\n",
    "\\text{w}_{1}^{\\top} \\\\\n",
    "\\text{w}_{2}^{\\top} \\\\\n",
    "... \\\\\n",
    "\\text{w}_{d}^{\\top}\n",
    "\\end{bmatrix} \\text{where}\\ \\text{w}_{j}\\ \\in\\ \\mathbb{R}^{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Classify}(\\text{x})\\ = \\underset{j\\ \\in\\ \\{1,\\ ...,\\ d\\}}{\\text{arg max softmax}}(\\text{Wx}\\ +\\ \\text{b})_{j} \\\\\n",
    "\n",
    "=\\ \\underset{j\\ \\in\\ \\{1,\\ ...,\\ d\\}}{\\text{arg max}}(\\text{Wx}\\ +\\ \\text{b})_{j} \\\\\n",
    "\n",
    "=\\ \\underset{j\\ \\in\\ \\{1,\\ ...,\\ d\\}}{\\text{arg max}}\\ \\text{w}_{j}^{\\top}\\text{x}\\ +\\ \\text{b}_{j}\n",
    "$$\n",
    "\n",
    "* Softmax maintains order of input regardless of linearity or non-linearity\n",
    "* run the input through softmax to find the max class label for a specific element\n",
    "* this run through softmax does not change which element has the highest value\n",
    "* softmax ensures the ouputs are probabilities somewhat interpretable over multiple classes without changing order\n",
    "* creates a multi directional plane of separation, one for each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Multi-Class Classification: Example\n",
    "\n",
    "Input $x$: day of the week\n",
    "\n",
    "Output: $f(x)$: precipitation (rain, snow, hail, sun)\n",
    "\n",
    "Prediction:\n",
    "* $P\\text{(rain)}\\ =\\ f_{\\theta}(\\text{x})_{1}\\ =\\ \\text{softmax}(\\text{Wx}\\ +\\ \\text{b})_{1}$\n",
    "* $P\\text{(snow)}\\ =\\ f_{\\theta}(\\text{x})_{2}\\ =\\ \\text{softmax}(\\text{Wx}\\ +\\ \\text{b})_{2}$\n",
    "* $P\\text{(hail)}\\ =\\ f_{\\theta}(\\text{x})_{3}\\ =\\ \\text{softmax}(\\text{Wx}\\ +\\ \\text{b})_{3}$\n",
    "* $P\\text{(sun)}\\ =\\ f_{\\theta}(\\text{x})_{4}\\ =\\ \\text{softmax}(\\text{Wx}\\ +\\ \\text{b})_{4}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1110, 0.2620, 0.2279, 0.3990], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LinearClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, n_classes) -> None:\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Linear(input_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return nn.functional.softmax(self.fc(x), dim=-1)\n",
    "    \n",
    "model = LinearClassifier(10, 4)\n",
    "x = torch.ones(10)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(20, 10)\n",
    "model(x)\n",
    "model(x).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Just like with binary classification, never define your model with softmax inside of the model\n",
    "* It is numeriocally unstable and softmax should be produced outside when training neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class vs Multiple Binary Classification\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "  <div style=\"width: 45%;\">\n",
    "    <h5>Multi-Class Classification (Softmax):</h3>\n",
    "    <ul>\n",
    "      <li>Descibes exactly <strong>one category</strong></li>\n",
    "      <li><strong>no negative</strong> examples</li>\n",
    "      <li>calibrated probabilities</li>\n",
    "      <li>used for <strong>mutually exclusive</strong> categories</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"width: 45%;\">\n",
    "    <h5>Multiple Binary Classifier (Sigmoid):</h3>\n",
    "    <ul>\n",
    "      <li>Allows for <strong>multiple categories</strong></li>\n",
    "      <li><strong>requires negative</strong> examples</li>\n",
    "      <li><strong>uncalibrated probabilities</strong></li>\n",
    "      <li>used for <strong>multi-label tagging</strong></li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between; margin-top: 20px;\">\n",
    "  <div style=\"width: 45%;\">\n",
    "    <strong>Examples:</strong>\n",
    "    <ul>\n",
    "      <li>Predicting the weather (rain, cloudy, sunny)</li>\n",
    "      <li>Predicting the scientific name of an animal</li>\n",
    "      <li>Predicting the next word in a sentence</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "  <div style=\"width: 45%;\">\n",
    "    <strong>Examples:</strong>\n",
    "    <ul>\n",
    "      <li>Predicting where in Texas it will rain</li>\n",
    "      <li>Predicting attributes of an animal</li>\n",
    "      <li>Predicting which books a sentence can be found in</li>\n",
    "    </ul>\n",
    "  </div>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
