{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3.4: Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap: Output Transformations\n",
    "\n",
    "Input: $\\text{x}$\n",
    "\n",
    "Output: $\\text{o}$\n",
    "\n",
    "Output transformation: $g$\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{...}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{g}\\ \\rightarrow\\ \\hat{\\text{y}}$\n",
    "\n",
    "where $f_{\\theta}$ lies within the linear computation layers and $\\psi$ lies within all interim layers before computing $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{...}\\ \\text{Linear}\\ \\rightarrow\\ \\text{Loss}\\ \\leftarrow\\ \\text{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{...}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{g}\\ \\rightarrow\\ \\hat{\\text{y}}$\n",
    "\n",
    "where $f_{\\theta}$ lies within the linear computation layers and $\\psi$ lies within all interim layers before computing $\\hat{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap: Loss\n",
    "\n",
    "Loss function: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x}_{i},\\ \\text{y}_{i})$\n",
    "\n",
    "Expected Loss: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$L(\\theta\\ |\\ \\mathcal{D})\\ =\\ \\mathbb{E}_{(\\text{x},\\ \\text{y})\\ \\sim\\ \\mathcal{D}}[l(\\theta\\ |\\ \\text{x},\\ \\text{y})]$\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{...}\\ \\text{Linear}\\ \\rightarrow\\ \\text{Loss}\\ \\leftarrow\\ \\text{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "Regression: $\\psi\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{R}$\n",
    "\n",
    "L1 Loss: </br>\n",
    "$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ ||\\text{y}\\ -\\ \\text{o}||_{1}\\ =\\ ||\\text{y}\\ -\\ f_{\\theta}(\\text{x})||_{1}$\n",
    "\n",
    "L2 Loss: </br>\n",
    "$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ ||\\text{y}\\ -\\ \\text{o}||_{2}^{2}\\ =\\ ||\\text{y}\\ -\\ f_{\\theta}(\\text{x})||_{2}^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification\n",
    "\n",
    "Binary classification $\\psi\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ [0,\\ 1]$\n",
    "* labels $y\\ \\in\\ \\{0,\\ 1\\}$\n",
    "\n",
    "Likelihood estimation\n",
    "* $p(0)\\ =\\ 1\\ -\\ \\sigma(f_{\\theta}(x))$\n",
    "* $p(1)\\ =\\ \\sigma(f_{\\theta}(x))$\n",
    "\n",
    "Binary cross entroy (negative log-likelihood) </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ \\ =\\ -\\text{log}\\ p(y)$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;$=\\ -[y\\ \\text{log}\\ p(1)\\ +\\ (1\\ -\\ y)\\ \\text{log}\\ p(0)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Classification Loss in Practice\n",
    "\n",
    "Numerical stability\n",
    "* $\\sigma(\\text{o})\\ =\\ 0\\ \\text{for}\\ o\\ \\rightarrow\\ -100$\n",
    "* $\\text{log}(\\sigma(o))\\ =\\ \\text{log}(0)\\ =\\ \\text{NaN !!}$\n",
    "\n",
    "Combine log and $\\sigma$ </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ -[y\\ \\text{log}\\ \\sigma(o)\\ +\\ (1\\ -\\ y)\\ \\text{log}\\ (1\\ -\\ \\sigma(o))]$\n",
    "* Use BCEWithLogitsLoss !!\n",
    "* Numerically more stable than Sigmoid + BCELoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class Classfication\n",
    "\n",
    "Binary classification $\\psi\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ [1,\\ ...,\\ C]$\n",
    "* labels $y \\in\\ \\{1,\\ ...,\\ C\\}$\n",
    "\n",
    "Likelihood estimation </br>\n",
    "$$\n",
    "\\text{p}\\ =\\ \\text{softmax}(\\text{o})\\ =\\ \n",
    "\\begin{bmatrix}\n",
    "p(1) \\\\\n",
    "p(2) \\\\\n",
    "\\vdots \\\\\n",
    "p(C)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Cross entropy (negative log-likelihood) </br>\n",
    "$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ -\\text{log}\\ p(y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Class Classification Loss in Practice\n",
    "\n",
    "Numerical stability\n",
    "* $\\text{softmax}(o)_{i}\\ \\rightarrow\\ 0\\ \\text{for}\\ o_{j}\\ -\\ o_{i}\\ >\\ 100$\n",
    "* $\\text{log}(\\text{softmax}(o)_{i})\\ =\\ \\text{log}(0)\\ \\text{is Nan}$\n",
    "\n",
    "Combine log and softmax </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$l(\\theta\\ |\\ \\text{x},\\ \\text{y})\\ =\\ -\\text{log softmax}(\\text{o})_{y}$\n",
    "* Use CrossEntropyLoss !!\n",
    "* numerically more stable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Functions - TL;DR\n",
    "* **Regression**: L1 loss `torch.nn.L1Loss`, L2 loss `torch.nn.MSELoss`\n",
    "* **Binary Classification**: binary cross-entropy loss `torch.nn.BCEWithLogitsLoss`\n",
    "* **Multi-Class Classification**: cross-entropy loss `torch.nn.CrossEntropyLoss`\n",
    "* **Always** use PyTorch loss for better numerical stability!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
