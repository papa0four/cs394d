{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3.2: Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recap: Non-linearities\n",
    "\n",
    "Rectified Linear Unit (ReLU) </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{ReLU}(x)\\ =\\ \\text{max}(x,\\ 0)$\n",
    "* Pro: Simple\n",
    "* Con: ReLU units can be fragile during training and \"die\"\n",
    "\n",
    "Dead ReLUs - How can we prevent dead ReLUs?\n",
    "* Initialize network carefully\n",
    "* Decrease the learning rate\n",
    "\n",
    "Leaky ReLU </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{LeakyReLU}(x)\\ =\\ \\text{max}(x,\\ \\alpha x)$\n",
    "* Where $0\\ <\\ \\alpha\\ <\\ 1$\n",
    "* called **PReLU** if $\\alpha$ is learned\n",
    "* Pro: Non-negative gradient for negative inputs\n",
    "* Con: The slope $\\alpha$ needs to be tuned\n",
    "* Con: Cannot wipe the negative signal out\n",
    "\n",
    "Exponential Linear Unit (Elu) </br>\n",
    "$$\n",
    "\\text{ELU}(x) = \\begin{cases}\n",
    "x & \\text{if}\\ x \\ge\\ 0 \\\\\n",
    "\\alpha(e^{x}\\ -\\ 1) & \\text{if}\\ x\\ <\\ 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Pro: Non-negative gradient for negative inputs\n",
    "* Con: $\\alpha$ needs to be tuned\n",
    "* Con: Exponential is computationally expensive\n",
    "\n",
    "Gaussian Linear Unit (GeLU) </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{GeLU}(x)\\ =\\ x\\ \\times\\ \\Phi (x)$\n",
    "* Where $\\Phi (x)$ is the CDF of the standard Gaussian\n",
    "* $\\Phi (x)\\ =\\ \\frac{1}{\\sqrt{2 \\pi}}\\ \\int_{- \\infty}^{x}\\ e^{- \\frac{t^{2}}{2}}dt$\n",
    "* Pro: Non-zero gradient for negative inputs\n",
    "* Con: Requires more computation\n",
    "\n",
    "Sigmoid </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\sigma (x)\\ =\\ \\frac{1}{1\\ +\\ e^{-x}}$\n",
    "* Same as $\\text{tanh} (x)\\ =\\ \\frac{e^{x}\\ -\\ e^{-x}}{e^{x}\\ +\\ e^{-x}}$\n",
    "* Con: Saturates on both ends\n",
    "* Do **not** use sigmoid/tanh\n",
    "\n",
    "Allows deep networks to model arbitrary differntiable functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions - TL;DR\n",
    "\n",
    "* Use ReLU with careful initialization and small learning rate\n",
    "* If ReLU fails, try Leaky ReLU or PReLU\n",
    "* Avoid Sigmoid and Tanh\n",
    "* Use GeLU for sophisticated models"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
