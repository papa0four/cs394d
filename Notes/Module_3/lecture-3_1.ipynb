{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3.1: Nonlinearities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Networks\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow \\text{o}$\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{o}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinearities\n",
    "\n",
    "Rectified Linear Unit (ReLU) </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{ReLU}(x)\\ =\\ \\text{max}(x,\\ 0)$\n",
    "\n",
    "Non-linear and differentiable almost everywhere </br>\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{o}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Simple Example - Why?\n",
    "\n",
    "Intuition </br>\n",
    "* first layer learns the color categories of the paw\n",
    "    * (white, black, gray)\n",
    "* second layer classifies color as paw or not\n",
    "\n",
    "How?\n",
    "\n",
    "$f(x)\\ =\\ \\text{ReLU}(x\\ -\\ \\frac{1}{2})\\ +\\ \\text{ReLU}(\\frac{1}{2}\\ -\\ x)\\ -\\ \\frac{1}{4}$\n",
    "\n",
    "$=\\ |x\\ -\\ \\frac{1}{2}|\\ -\\ \\frac{1}{4}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Networks </br>\n",
    "Alterantes linear and non-linear layers\n",
    "\n",
    "$x \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{o}$\n",
    "\n",
    "Model: $f_{\\theta}\\ :\\ \\mathbb{R}^{n}\\ \\rightarrow\\ \\mathbb{R}^{c}$\n",
    "\n",
    "Parameters: $\\theta\\ =\\ (\\text{W}_{1},\\ \\text{b}_{1},\\ ...,\\ \\text{W}_{N},\\ \\text{b}_{N})$\n",
    "\n",
    "Computation: </br>\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{h}_{1}\\ =\\ \\text{ReLU}(\\text{W}_{1}\\text{x}\\ +\\ \\text{b}_{1})$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{h}_{2}\\ =\\ \\text{ReLU}(\\text{W}_{2}\\text{h}_{1}\\ +\\ \\text{b}_{2})$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;$\\vdots$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$\\text{h}_{N\\ -\\ 1}\\ =\\ \\text{ReLU}(\\text{W}_{N\\ -\\ 1}\\text{h}_{N\\ -\\ 2}\\ +\\ \\text{b}_{N\\ -\\ 1})$\n",
    "\n",
    "&ensp;&ensp;&ensp;&ensp;$f_{\\theta}(\\text{x})\\ =\\ \\text{W}_{N}\\text{h}_{N\\ -\\ 1}\\ +\\ \\text{b}_{N}$\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow \\text{o}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is a layer?\n",
    "\n",
    "Largest computational unit that remains unchanged throughout different architectures </br>\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{o}$\n",
    "\n",
    "Input: $x$\n",
    "\n",
    "Linear Layer: $w\\ \\rightarrow\\ \\text{mat mul}\\ \\rightarrow\\ \\text{add}\\ \\leftarrow\\ \\text{b}$\n",
    "\n",
    "Output: $\\rightarrow\\ \\text{o}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many layers does a deep network have?\n",
    "\n",
    "We only count linear layers </br>\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{ReLU}\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{o}$\n",
    "\n",
    "$x\\ \\rightarrow\\ \\text{Linear}\\ \\rightarrow\\ \\text{o}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Universal Approximation Theorem\n",
    "\n",
    "**Universal Approximation Theorem**\n",
    "A two-layer deep network can approximate any continuous function.\n",
    "* Constructing is inefficient\n",
    "* Deep learning exploit structure in data to find efficient approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linearities - TL;DR\n",
    "\n",
    "Deep networks are stacks of alternating linear and non-linear layers\n",
    "\n",
    "Deep networks belong to a class of continuous functions that can approximate **any** continuous function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
